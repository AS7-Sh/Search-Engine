{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd0917d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class LemmatizerWithPOSTagger(WordNetLemmatizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _get_wordnet_pos(self, tag: str) -> str:\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemmatize(self, word: str, pos: str = \"n\") -> str:\n",
    "        return super().lemmatize(word, self._get_wordnet_pos(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bacdbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from typing import Callable\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "import numpy as np\n",
    "from nltk import tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import PorterStemmer\n",
    "from num2words import num2words\n",
    "from nltk.corpus import wordnet\n",
    "from dateutil import parser\n",
    "import nltk\n",
    "\n",
    "class TextPreprocessor():\n",
    "\n",
    "    def __init__(self, tokenizer: Callable = None) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            self.tokenizer = tokenize.word_tokenize\n",
    "\n",
    "        self.stopwords_tokens = stopwords.words('english')\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.dateNormalizer=NormalizerDates()\n",
    "        self.lemmatizer = LemmatizerWithPOSTagger()\n",
    "\n",
    "    def tokenize(self, text: str)-> List[str]:\n",
    "        tokens =self.tokenizer(text)\n",
    "        return tokens\n",
    "    \n",
    "    def to_lower(self, tokens: List[str]) -> List[str]:\n",
    "        lower_tokens = []\n",
    "        for token in tokens:\n",
    "            lower_token = str(np.char.lower(token))\n",
    "            lower_tokens.append(lower_token)\n",
    "        return lower_tokens\n",
    "\n",
    "    \n",
    "    def remove_markers(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(re.sub(r'\\u00AE', '', token))\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_punctuation(self, tokens: List[str]) ->  List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(token.translate(str.maketrans('', '', string.punctuation)))\n",
    "        return new_tokens\n",
    "\n",
    "\n",
    "    def correct_sentence_spelling(self, tokens: List[str]) -> List[str]:\n",
    "        spell = SpellChecker()\n",
    "        misspelled = spell.unknown(tokens)\n",
    "        for i,token in enumerate(tokens):\n",
    "            if token in misspelled :\n",
    "                corrected=spell.correction(token)\n",
    "                if(corrected!=None):\n",
    "                    tokens[i]=corrected\n",
    "        return tokens  \n",
    "\n",
    "    def rplace_under_score_with_space(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(re.sub(r'_', ' ', token))\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_stop_words(self,tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in self.stopwords_tokens and len(token) > 1:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "\n",
    "    def remove_apostrophe(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(str(np.char.replace(token, \"'\", \" \")))\n",
    "        return new_tokens\n",
    "\n",
    "    def stemming(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.append(self.stemmer.stem(token))\n",
    "        return new_tokens\n",
    "    \n",
    "    \n",
    "    def normalize_appreviations(self, tokens: List[str]) -> List[str]:\n",
    "        new_tokens = []\n",
    "        resolved_terms = {}\n",
    "        for token in tokens:\n",
    "\n",
    "            if len(token) >= 2:\n",
    "                synsets = wordnet.synsets(token)\n",
    "                if synsets:\n",
    "                    resolved_term = synsets[0].lemmas()[0].name()\n",
    "                    resolved_terms[token] = resolved_term\n",
    "\n",
    "        for abbreviation, resolved_term in resolved_terms.items():\n",
    "            for i in range(len(tokens)):\n",
    "                if tokens[i] == abbreviation:\n",
    "                    tokens[i] = resolved_term\n",
    "                    break\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def lemmatizing(self, tokens: List[str]) -> List[str]:\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token, pos) for token, pos in tagged_tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        operations = [\n",
    "            self.to_lower,\n",
    "            self.remove_punctuation,\n",
    "            self.remove_apostrophe,\n",
    "            self.remove_stop_words,\n",
    "            self.remove_markers,\n",
    "            self.stemming,\n",
    "            self.lemmatizing,\n",
    "            self.normalize_appreviations, \n",
    "            self.to_lower,\n",
    "            self.rplace_under_score_with_space\n",
    "        ]\n",
    "        text_tokens=self.tokenize(text)\n",
    "        for op in operations:\n",
    "              text_tokens=op(text_tokens)\n",
    "    \n",
    "        new_text=\"\"\n",
    "        new_text = ' '.join(text_tokens)\n",
    "            \n",
    "        return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95d8626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(document:str):\n",
    "    return TextPreprocessor().preprocess(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c07c1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize(documents):\n",
    "    documents_vectors = []\n",
    "    i=0\n",
    "    for document in documents:\n",
    "        zero_vector = np.zeros(500)\n",
    "        vectors = []\n",
    "        for token in document:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    vectors.append(np.random(500))\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            documents_vectors.append(avg_vec)\n",
    "        else:\n",
    "            documents_vectors.append(zero_vector)\n",
    "    return documents_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a03d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('D:/wikir/word2vec/documents_vectors.pickle', 'rb') as handle:\n",
    "    documents_vectors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "388f0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"localhost:27017\")\n",
    "\n",
    "db = client[\"IR\"]\n",
    "col = db[\"wikir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6339c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model=Word2Vec.load(\"D:/wikir/word2vec/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61060c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3a26a_row0_col0, #T_3a26a_row0_col1, #T_3a26a_row0_col2, #T_3a26a_row1_col0, #T_3a26a_row1_col1, #T_3a26a_row1_col2, #T_3a26a_row2_col0, #T_3a26a_row2_col1, #T_3a26a_row2_col2, #T_3a26a_row3_col0, #T_3a26a_row3_col1, #T_3a26a_row3_col2, #T_3a26a_row4_col0, #T_3a26a_row4_col1, #T_3a26a_row4_col2, #T_3a26a_row5_col0, #T_3a26a_row5_col1, #T_3a26a_row5_col2, #T_3a26a_row6_col0, #T_3a26a_row6_col1, #T_3a26a_row6_col2, #T_3a26a_row7_col0, #T_3a26a_row7_col1, #T_3a26a_row7_col2, #T_3a26a_row8_col0, #T_3a26a_row8_col1, #T_3a26a_row8_col2, #T_3a26a_row9_col0, #T_3a26a_row9_col1, #T_3a26a_row9_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3a26a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3a26a_level0_col0\" class=\"col_heading level0 col0\" >_id</th>\n",
       "      <th id=\"T_3a26a_level0_col1\" class=\"col_heading level0 col1\" >index</th>\n",
       "      <th id=\"T_3a26a_level0_col2\" class=\"col_heading level0 col2\" >content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3a26a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3a26a_row0_col0\" class=\"data row0 col0\" >1742906</td>\n",
       "      <td id=\"T_3a26a_row0_col1\" class=\"data row0 col1\" >145236</td>\n",
       "      <td id=\"T_3a26a_row0_col2\" class=\"data row0 col2\" >for his contributions to american railroad management jeffery is listed by the smithsonian institutions john h white jr as one of america s most noteworthy railroaders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a26a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3a26a_row1_col0\" class=\"data row1 col0\" >1511320</td>\n",
       "      <td id=\"T_3a26a_row1_col1\" class=\"data row1 col1\" >13199</td>\n",
       "      <td id=\"T_3a26a_row1_col2\" class=\"data row1 col2\" >it operates several chains of retail brands in the consumer durables sector specializing in furniture audio video appliances and electronics in over 1 000 stores in central america the caribbean south america and the united states employing over 15 000 associates the unicomer group was founded in 2000 unicomer group owns large brands such as la cura ao central america and the dominican republic almacenes tropigas in central america gollo in costa rica artefacta in ecuador and electro facil in paraguay in the caribbean region unicomer group operates through its retail brand courts other brands in the region are lucky dollar omni amc unicon among others unicomer usa and courts caribbean located in the united states as of april 15 2015 unicomer group acquired brands intellectual property and contracts of existing radioshack franchisees throughout central america south america and the caribbean holding a promise to expand presence of this chain in these regions as new operations and with existing franchisees since the year 2000 unicomer group has grown from operating 4 store chains in central america to operating more than 30 brands in 26 countries in central america south america the caribbean islands and usa the central american retail chain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a26a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3a26a_row2_col0\" class=\"data row2 col0\" >1820054</td>\n",
       "      <td id=\"T_3a26a_row2_col1\" class=\"data row2 col1\" >112357</td>\n",
       "      <td id=\"T_3a26a_row2_col2\" class=\"data row2 col2\" >startech com services a worldwide market with operations throughout the united states canada europe latin america and taiwan the company headquarters is located in london ontario canada with distribution centers in the united states canada the united kingdom and singapore startech com was founded in 1985 in london ontario canada by paul seed and ken kalopsis the company s first products to enter the it market were anti glare screens for crt computer monitors and keyboard dust covers although startech com has been active in the canadian and united states it markets since the company s beginning it was not until 2004 that startech com decided to focus on becoming a more globalized company with the opening of a branch in northampton uk in 2010 the company further expanded their uk operation with the appointment of a business manager uk country manager and national account manager the same year both the uk and usa warehouses were relocated to better accommodate demand for products by 2012 startech com was selling products in many european markets including france spain italy benelux as well as mexico in 2019 startech com was selling in 20 countries worldwide and has plans for further expansion the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a26a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3a26a_row3_col0\" class=\"data row3 col0\" >775669</td>\n",
       "      <td id=\"T_3a26a_row3_col1\" class=\"data row3 col1\" >219988</td>\n",
       "      <td id=\"T_3a26a_row3_col2\" class=\"data row3 col2\" >the central and southern portions of the continent are represented by the united states mexico and numerous smaller states primarily in central america and in the caribbean the continent is delimited on the southeast by most geographers at the dari n watershed along the colombia panama border placing all of panama within north america alternatively a less common view would end north america at the man made panama canal islands generally associated with north america include greenland the world s largest island and archipelagos and islands in the caribbean the terminology of the americas is complex but anglo america can describe canada and the u s while latin america comprises mexico and the countries of central america and the caribbean as well as the entire continent of south america natural features of north america include the northern portion of the american cordillera represented by the geologically new rocky mountains in the west and the considerably older appalachian mountains to the east the north hosts an abundance of glacial lakes formed during the last glacial period including the great lakes north america s major continental divide is the great divide which runs north and south down through rocky mountains the major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a26a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3a26a_row4_col0\" class=\"data row4 col0\" >1146970</td>\n",
       "      <td id=\"T_3a26a_row4_col1\" class=\"data row4 col1\" >241090</td>\n",
       "      <td id=\"T_3a26a_row4_col2\" class=\"data row4 col2\" >coach america consisted of all former coach usa operations except for the midwestern united states new york new jersey pennsylvania and new england along with lakefront lines in ohio acquired separately for the nine years of its existence coach america was based in dallas texas the properties that became coach america were previously owned by scotland based stagecoach group as coach usa s western south central and southeastern divisions coach america was formed in 2003 when after stagecoach group evaluated its coach usa business it decided to retain mostly its scheduled and local transit services in the northeast and north central region and put the rest of the company up for sale the south central and west divisions of coach usa were sold to kohlberg co llc with these companies continuing to use the coach usa name for a time but eventually changing to coach america the southeast division was sold to a separate buyer lincolnshire management and became american coach lines in 2006 coach america purchased american coach lines from lincolnshire in november of that same year kohlberg sold coach america to another private equity firm fenway partners coach america acquired the ohio carrier lakefront lines in 2008 in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a26a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_3a26a_row5_col0\" class=\"data row5 col0\" >225121</td>\n",
       "      <td id=\"T_3a26a_row5_col1\" class=\"data row5 col1\" >127741</td>\n",
       "      <td id=\"T_3a26a_row5_col2\" class=\"data row5 col2\" >the canadian province of quebec is the centre of the community and is the point of origin of most of french america it also includes communities in all provinces of canada especially in new brunswick where francophones are roughly one third of the population saint pierre and miquelon haiti saint martin saint barth lemy saint lucia martinique and guadeloupe in the caribbean french guiana overseas region of france in south america also there are minorities of french speakers in part of the united states new england louisiana florida dominica grenada and trinidad and tobago the ordre des francophones d am rique is a decoration given in the name of the community to its members it can also be described as the francophonie of the americas because french is a romance language french america is sometimes considered to be part of latin america but this term more often refers to hispanic america and portuguese america or simply the americas south of the united states this is a list of countries administrative divisions and french possessions in the americas having the french language as an official language the data of each place are based in the 2012 2013 census</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a26a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_3a26a_row6_col0\" class=\"data row6 col0\" >1645290</td>\n",
       "      <td id=\"T_3a26a_row6_col1\" class=\"data row6 col1\" >341617</td>\n",
       "      <td id=\"T_3a26a_row6_col2\" class=\"data row6 col2\" >from 2000 through 2013 he was professor of music at the university of southampton subsequently having become entirely disenchanted with the uk s tertiary educational system he took early retirement and was awarded the title of emeritus professor of music though he later relinquished this post from 1987 to 2000 he was professor of music and sometime research dean of the faculty of humanities at keele university between 1984 and 1987 nicholls was keasbey fellow in american studies at selwyn college cambridge in 1998 he spent an extended semester at the college of william and mary in virginia usa as visiting professor of music during his academic career nicholls gave many presentations both refereed and guest in north america france germany mexico australia taiwan and the united kingdom he is a former editor of the journal american music 2000 2005 until 2000 nicholls was also active as a composer and his works were performed and broadcast in the united kingdom europe america australia and south africa nicholls was a pupil at st benedict s primary school in small heath birmingham and subsequently king edward vi camp hill school for boys in king s heath he then read music at st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a26a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_3a26a_row7_col0\" class=\"data row7 col0\" >1777377</td>\n",
       "      <td id=\"T_3a26a_row7_col1\" class=\"data row7 col1\" >212325</td>\n",
       "      <td id=\"T_3a26a_row7_col2\" class=\"data row7 col2\" >tilyard went on to produce battleground with ayz waraich the 80 s style cult action film was released internationally in 2011 and is scheduled for release in north america in 2012 official site www dimeworth com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a26a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_3a26a_row8_col0\" class=\"data row8 col0\" >1200775</td>\n",
       "      <td id=\"T_3a26a_row8_col1\" class=\"data row8 col1\" >188504</td>\n",
       "      <td id=\"T_3a26a_row8_col2\" class=\"data row8 col2\" >the show runs for two hours on sundays from 9 to 11 a m pacific time dhuyvetter typically broadcasts from her studio in encinitas california but often takes the show on the road she has broadcast live from remote sites in asia africa europe south america and north america traveltalkradio focuses on travel and tourism and began airing in 2001 each week host dhuyvetter speaks with experts from around the world in the travel industry as well as authors of travel books and publishers of travel magazines the program introduces audiences to regional national and international travel topics with the goal of facilitating communications between tour operators wholesales travel agents and consumers dhuyvetter is quoted as saying that traveltalkradio was one of the first internet radio shows traveltalkradio is affiliated with traveltalkmedia which encompasses the web television and radio aspects of the company the show is distributed by conventional radio stations in the united states united kingdom africa and china including on china national radio via satellite traveltalkradio is distributed in the western hemisphere south america and the pacific rim traveltalkradio and traveltalkmedia fall under the parent company celestialink llc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a26a_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_3a26a_row9_col0\" class=\"data row9 col0\" >2159177</td>\n",
       "      <td id=\"T_3a26a_row9_col1\" class=\"data row9 col1\" >248401</td>\n",
       "      <td id=\"T_3a26a_row9_col2\" class=\"data row9 col2\" >in the western hemisphere this took the form of organizing against the expansion of american commercial influence in the developing nations of central and south america as well as the caribbean basin including especially mexico the dominican republic cuba and nicaragua in the united states itself the anti imperialist department of the well funded workers communist party of america was charles shipman 1895 1989 a draft resisting american expatriate to mexico who as jes s ram rez had been a delegate representing that country at the 2nd world congress of the comintern in addition to latin american concerns shipman s department had also propagandized against american commercial and military involvement in other parts of the globe including particularly the philippines and china in april 1925 shipman was dispatched to mexico as the representative of the workers party to the 3rd congress of the communist party of mexico it was at this time that a new international organization was launched the all america anti imperialist league an organization which would eventually include national sections throughout latin america the term all america in the organizational moniker was not intended to relate specifically to the united states but rather to the fact that</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22bd0cfb3d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "def get_results(query_fin):\n",
    "    similarities = cosine_similarity(documents_vectors, vectorize([word_tokenize(process_text(query_fin))])[0].reshape(1, -1))\n",
    "\n",
    "    sorted_indices = similarities.argsort(axis=0)[-10:][::-1].flatten()\n",
    "    result_ids= []\n",
    "    \n",
    "    for i in sorted_indices:\n",
    "        if(similarities[i][0]>=0.35):\n",
    "            result_ids.append(int(i))\n",
    "\n",
    "    unordered_results= list(col.find({'index':{'$in':result_ids} }))\n",
    "    \n",
    "    return sorted(unordered_results, key=lambda x: result_ids.index(x['index']))\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "df=pd.DataFrame(get_results('america'))\n",
    "df = df.style.set_properties(**{'text-align': 'left'})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0214e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = {}\n",
    "with open('D:/wikir/test/qrels','r')as f:\n",
    "    for line in f:\n",
    "        query_id, _, doc_id, relevance = line.strip().split()\n",
    "        if query_id not in qrels:\n",
    "            qrels[query_id] = {}\n",
    "        qrels[query_id][doc_id] = int(relevance)\n",
    "# Create a dataset object from the qrels dictionary\n",
    "from collections import defaultdict\n",
    "dataset = defaultdict(dict)\n",
    "for query_id, doc_dict in qrels.items():\n",
    "    for doc_id, relevance in doc_dict.items():\n",
    "        dataset[query_id][doc_id] = relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ce22ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5147344104308391\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def get_query(query_id):\n",
    "    with open('D:/wikir/test/queries.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if row[0] == query_id:\n",
    "                return row[1] \n",
    "    \n",
    "    \n",
    "def calculate_MAP(query_id):\n",
    "    relevant_docs =[]\n",
    "    for qrel in dataset.items():\n",
    "        if qrel[0] == query_id :\n",
    "            for key, value in qrel[1].items():\n",
    "                relevant_docs.append(key)\n",
    "    \n",
    "    ordered_results=[]\n",
    "    for query in dataset.items():\n",
    "        if query[0] == query_id:\n",
    "            ordered_results=get_results(get_query(query_id))\n",
    "            break\n",
    "    \n",
    "    pk_sum=0\n",
    "    total_relevant=0\n",
    "    for i in range(1,11):\n",
    "        relevant_ret=0\n",
    "        for j in range(i):\n",
    "            if(j<len(ordered_results) and ordered_results[j]['_id'] in relevant_docs):\n",
    "                relevant_ret += 1\n",
    "        p_at_k= (relevant_ret/(i)) * (1 if i-1<len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs else 0)\n",
    "        pk_sum+=p_at_k\n",
    "        if(i-1<len(ordered_results) and ordered_results[i-1]['_id'] in relevant_docs):\n",
    "            total_relevant+=1\n",
    "    \n",
    "    return 0 if total_relevant==0 else pk_sum/total_relevant\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "queries_ids={}\n",
    "for qrel in dataset.items():\n",
    "    queries_ids.update({qrel[0]:''})\n",
    "\n",
    "    \n",
    "map_sum=0\n",
    "for query_id in list(queries_ids.keys()):\n",
    "     map_sum+= calculate_MAP(query_id)\n",
    "\n",
    "print(map_sum/len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c79630ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 158491, Recall@10: 0.1111111111111111\n",
      "Query ID: 158491, Precision@10: 0.1\n",
      "Query ID: 5728, Recall@10: 0.13333333333333333\n",
      "Query ID: 5728, Precision@10: 0.2\n",
      "Query ID: 13554, Recall@10: 0.0\n",
      "Query ID: 13554, Precision@10: 0.0\n",
      "Query ID: 32674, Recall@10: 0.8571428571428571\n",
      "Query ID: 32674, Precision@10: 0.6\n",
      "Query ID: 406391, Recall@10: 0.125\n",
      "Query ID: 406391, Precision@10: 0.1\n",
      "Query ID: 5115, Recall@10: 0.125\n",
      "Query ID: 5115, Precision@10: 0.1\n",
      "Query ID: 15469, Recall@10: 0.08333333333333333\n",
      "Query ID: 15469, Precision@10: 0.2\n",
      "Query ID: 62953, Recall@10: 0.3333333333333333\n",
      "Query ID: 62953, Precision@10: 0.2\n",
      "Query ID: 152444, Recall@10: 0.1111111111111111\n",
      "Query ID: 152444, Precision@10: 0.1\n",
      "Query ID: 104086, Recall@10: 0.2\n",
      "Query ID: 104086, Precision@10: 0.2\n",
      "Query ID: 145194, Recall@10: 0.16666666666666666\n",
      "Query ID: 145194, Precision@10: 0.1\n",
      "Query ID: 73752, Recall@10: 0.0\n",
      "Query ID: 73752, Precision@10: 0.0\n",
      "Query ID: 1368508, Recall@10: 0.12195121951219512\n",
      "Query ID: 1368508, Precision@10: 0.5\n",
      "Query ID: 11534, Recall@10: 0.15\n",
      "Query ID: 11534, Precision@10: 0.3\n",
      "Query ID: 83078, Recall@10: 0.0\n",
      "Query ID: 83078, Precision@10: 0.0\n",
      "Query ID: 25174, Recall@10: 0.012987012987012988\n",
      "Query ID: 25174, Precision@10: 0.1\n",
      "Query ID: 265104, Recall@10: 0.25\n",
      "Query ID: 265104, Precision@10: 0.4\n",
      "Query ID: 139082, Recall@10: 0.14285714285714285\n",
      "Query ID: 139082, Precision@10: 0.1\n",
      "Query ID: 37743, Recall@10: 0.16666666666666666\n",
      "Query ID: 37743, Precision@10: 0.1\n",
      "Query ID: 207224, Recall@10: 0.0\n",
      "Query ID: 207224, Precision@10: 0.0\n",
      "Query ID: 1313611, Recall@10: 0.14285714285714285\n",
      "Query ID: 1313611, Precision@10: 0.1\n",
      "Query ID: 641464, Recall@10: 0.09090909090909091\n",
      "Query ID: 641464, Precision@10: 0.1\n",
      "Query ID: 75295, Recall@10: 0.25\n",
      "Query ID: 75295, Precision@10: 0.2\n",
      "Query ID: 2591, Recall@10: 0.0\n",
      "Query ID: 2591, Precision@10: 0.0\n",
      "Query ID: 107590, Recall@10: 0.14285714285714285\n",
      "Query ID: 107590, Precision@10: 0.1\n",
      "Query ID: 362197, Recall@10: 0.3333333333333333\n",
      "Query ID: 362197, Precision@10: 0.2\n",
      "Query ID: 13540, Recall@10: 0.0007249003262051468\n",
      "Query ID: 13540, Precision@10: 0.2\n",
      "Query ID: 25406, Recall@10: 0.13333333333333333\n",
      "Query ID: 25406, Precision@10: 0.2\n",
      "Query ID: 128282, Recall@10: 0.08333333333333333\n",
      "Query ID: 128282, Precision@10: 0.1\n",
      "Query ID: 267973, Recall@10: 0.75\n",
      "Query ID: 267973, Precision@10: 0.6\n",
      "Query ID: 116217, Recall@10: 0.09090909090909091\n",
      "Query ID: 116217, Precision@10: 0.1\n",
      "Query ID: 16130, Recall@10: 0.1\n",
      "Query ID: 16130, Precision@10: 0.2\n",
      "Query ID: 471746, Recall@10: 0.125\n",
      "Query ID: 471746, Precision@10: 0.1\n",
      "Query ID: 113000, Recall@10: 0.6666666666666666\n",
      "Query ID: 113000, Precision@10: 0.4\n",
      "Query ID: 87860, Recall@10: 0.01092896174863388\n",
      "Query ID: 87860, Precision@10: 0.2\n",
      "Query ID: 25983, Recall@10: 0.36363636363636365\n",
      "Query ID: 25983, Precision@10: 0.4\n",
      "Query ID: 11919, Recall@10: 0.11538461538461539\n",
      "Query ID: 11919, Precision@10: 0.3\n",
      "Query ID: 22343, Recall@10: 0.5714285714285714\n",
      "Query ID: 22343, Precision@10: 0.4\n",
      "Query ID: 81083, Recall@10: 0.0\n",
      "Query ID: 81083, Precision@10: 0.0\n",
      "Query ID: 172877, Recall@10: 0.0\n",
      "Query ID: 172877, Precision@10: 0.0\n",
      "Query ID: 7169, Recall@10: 0.14285714285714285\n",
      "Query ID: 7169, Precision@10: 0.1\n",
      "Query ID: 13690, Recall@10: 0.16666666666666666\n",
      "Query ID: 13690, Precision@10: 0.1\n",
      "Query ID: 8140, Recall@10: 0.5\n",
      "Query ID: 8140, Precision@10: 0.4\n",
      "Query ID: 206019, Recall@10: 0.0\n",
      "Query ID: 206019, Precision@10: 0.0\n",
      "Query ID: 104453, Recall@10: 0.0\n",
      "Query ID: 104453, Precision@10: 0.0\n",
      "Query ID: 73673, Recall@10: 0.25\n",
      "Query ID: 73673, Precision@10: 0.2\n",
      "Query ID: 996687, Recall@10: 0.3333333333333333\n",
      "Query ID: 996687, Precision@10: 0.2\n",
      "Query ID: 121384, Recall@10: 0.375\n",
      "Query ID: 121384, Precision@10: 0.3\n",
      "Query ID: 87686, Recall@10: 0.7142857142857143\n",
      "Query ID: 87686, Precision@10: 0.5\n",
      "Query ID: 410859, Recall@10: 0.3333333333333333\n",
      "Query ID: 410859, Precision@10: 0.2\n",
      "Query ID: 108909, Recall@10: 0.18181818181818182\n",
      "Query ID: 108909, Precision@10: 0.2\n",
      "Query ID: 92044, Recall@10: 0.03225806451612903\n",
      "Query ID: 92044, Precision@10: 0.1\n",
      "Query ID: 371437, Recall@10: 0.07142857142857142\n",
      "Query ID: 371437, Precision@10: 0.1\n",
      "Query ID: 151303, Recall@10: 0.1\n",
      "Query ID: 151303, Precision@10: 0.3\n",
      "Query ID: 101626, Recall@10: 0.1111111111111111\n",
      "Query ID: 101626, Precision@10: 0.2\n",
      "Query ID: 79032, Recall@10: 0.0\n",
      "Query ID: 79032, Precision@10: 0.0\n",
      "Query ID: 17656, Recall@10: 0.09090909090909091\n",
      "Query ID: 17656, Precision@10: 0.1\n",
      "Query ID: 15514, Recall@10: 0.023809523809523808\n",
      "Query ID: 15514, Precision@10: 0.1\n",
      "Query ID: 1254734, Recall@10: 0.3\n",
      "Query ID: 1254734, Precision@10: 0.3\n",
      "Query ID: 1238278, Recall@10: 0.13636363636363635\n",
      "Query ID: 1238278, Precision@10: 0.3\n",
      "Query ID: 21645, Recall@10: 0.0625\n",
      "Query ID: 21645, Precision@10: 0.1\n",
      "Query ID: 1280405, Recall@10: 0.0\n",
      "Query ID: 1280405, Precision@10: 0.0\n",
      "Query ID: 32559, Recall@10: 0.0\n",
      "Query ID: 32559, Precision@10: 0.0\n",
      "Query ID: 34939, Recall@10: 0.16666666666666666\n",
      "Query ID: 34939, Precision@10: 0.1\n",
      "Query ID: 174151, Recall@10: 0.1\n",
      "Query ID: 174151, Precision@10: 0.1\n",
      "Query ID: 4900, Recall@10: 0.043478260869565216\n",
      "Query ID: 4900, Precision@10: 0.1\n",
      "Query ID: 10166, Recall@10: 0.1111111111111111\n",
      "Query ID: 10166, Precision@10: 0.2\n",
      "Query ID: 25726, Recall@10: 0.028985507246376812\n",
      "Query ID: 25726, Precision@10: 0.2\n",
      "Query ID: 720, Recall@10: 0.0\n",
      "Query ID: 720, Precision@10: 0.0\n",
      "Query ID: 200237, Recall@10: 0.0\n",
      "Query ID: 200237, Precision@10: 0.0\n",
      "Query ID: 14410, Recall@10: 0.25\n",
      "Query ID: 14410, Precision@10: 0.7\n",
      "Query ID: 23920, Recall@10: 0.05\n",
      "Query ID: 23920, Precision@10: 0.1\n",
      "Query ID: 1897191, Recall@10: 0.0\n",
      "Query ID: 1897191, Precision@10: 0.0\n",
      "Query ID: 196387, Recall@10: 0.16666666666666666\n",
      "Query ID: 196387, Precision@10: 0.1\n",
      "Query ID: 25130, Recall@10: 0.3333333333333333\n",
      "Query ID: 25130, Precision@10: 0.2\n",
      "Query ID: 28232, Recall@10: 0.2857142857142857\n",
      "Query ID: 28232, Precision@10: 0.2\n",
      "Query ID: 16952, Recall@10: 0.4\n",
      "Query ID: 16952, Precision@10: 0.4\n",
      "Query ID: 28485, Recall@10: 0.14285714285714285\n",
      "Query ID: 28485, Precision@10: 0.1\n",
      "Query ID: 113827, Recall@10: 0.125\n",
      "Query ID: 113827, Precision@10: 0.1\n",
      "Query ID: 32219, Recall@10: 0.125\n",
      "Query ID: 32219, Precision@10: 0.1\n",
      "Query ID: 6984, Recall@10: 0.08333333333333333\n",
      "Query ID: 6984, Precision@10: 0.1\n",
      "Query ID: 124119, Recall@10: 0.0\n",
      "Query ID: 124119, Precision@10: 0.0\n",
      "Query ID: 34875, Recall@10: 0.1\n",
      "Query ID: 34875, Precision@10: 0.1\n",
      "Query ID: 7911, Recall@10: 0.1111111111111111\n",
      "Query ID: 7911, Precision@10: 0.1\n",
      "Query ID: 8630, Recall@10: 0.16666666666666666\n",
      "Query ID: 8630, Precision@10: 0.1\n",
      "Query ID: 422206, Recall@10: 0.42857142857142855\n",
      "Query ID: 422206, Precision@10: 0.3\n",
      "Query ID: 81416, Recall@10: 0.23809523809523808\n",
      "Query ID: 81416, Precision@10: 0.5\n",
      "Query ID: 28106, Recall@10: 0.18181818181818182\n",
      "Query ID: 28106, Precision@10: 0.2\n",
      "Query ID: 738573, Recall@10: 0.0\n",
      "Query ID: 738573, Precision@10: 0.0\n",
      "Query ID: 125791, Recall@10: 0.14285714285714285\n",
      "Query ID: 125791, Precision@10: 0.1\n",
      "Query ID: 294384, Recall@10: 0.14285714285714285\n",
      "Query ID: 294384, Precision@10: 0.1\n",
      "Query ID: 470620, Recall@10: 0.3333333333333333\n",
      "Query ID: 470620, Precision@10: 0.2\n",
      "Query ID: 237831, Recall@10: 0.16666666666666666\n",
      "Query ID: 237831, Precision@10: 0.1\n",
      "Query ID: 2161, Recall@10: 0.03636363636363636\n",
      "Query ID: 2161, Precision@10: 0.2\n",
      "Query ID: 21145, Recall@10: 0.02857142857142857\n",
      "Query ID: 21145, Precision@10: 0.2\n",
      "Query ID: 679227, Recall@10: 0.09523809523809523\n",
      "Query ID: 679227, Precision@10: 0.2\n",
      "Query ID: 2136797, Recall@10: 0.16666666666666666\n",
      "Query ID: 2136797, Precision@10: 0.1\n",
      "Query ID: 5622, Recall@10: 0.05263157894736842\n",
      "Query ID: 5622, Precision@10: 0.1\n",
      "Query ID: 1313598, Recall@10: 0.2\n",
      "Query ID: 1313598, Precision@10: 0.2\n",
      "Query ID: 712704, Recall@10: 0.09090909090909091\n",
      "Query ID: 712704, Precision@10: 0.3\n"
     ]
    }
   ],
   "source": [
    "def calculate_recall_precision(query_id):\n",
    "    relevant_docs =[]\n",
    "    for qrel in dataset.items():\n",
    "        if qrel[0] == query_id :\n",
    "            for key, value in qrel[1].items():\n",
    "                relevant_docs.append(key)\n",
    "\n",
    "    retrieved_docs=[]\n",
    "    for query in dataset.items():\n",
    "        if query[0] == query_id:\n",
    "            retrieved_docs=get_results(get_query(query_id))\n",
    "            break\n",
    "            \n",
    "    truncated_retrieved_docs = [obj['_id'] for obj in retrieved_docs[:len(relevant_docs)]]\n",
    "    y_true = [1 if result['_id'] in relevant_docs else 0 for i, result  in enumerate(retrieved_docs)]\n",
    "    true_positives = sum([1 for i in range(len(y_true)) if y_true[i]==1])\n",
    "    recall_at_10 = true_positives/len(relevant_docs)\n",
    "    precision_at_10 = true_positives / 10\n",
    "    print(f\"Query ID: {query_id}, Recall@10: {recall_at_10}\")\n",
    "    print(f\"Query ID: {query_id}, Precision@10: {precision_at_10}\")    \n",
    "    \n",
    "    return recall_at_10\n",
    "\n",
    "\n",
    "queries_ids = {}\n",
    "for qrel in dataset.items():\n",
    "    queries_ids.update({qrel[0]:''})\n",
    "\n",
    "mrr_sum = 0\n",
    "for query_id in list(queries_ids.keys()):\n",
    "    calculate_recall_precision(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82f89643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.585309523809524\n"
     ]
    }
   ],
   "source": [
    "def calculate_MRR(query_id):\n",
    "    relevant_docs =[]\n",
    "    for qrel in dataset.items():\n",
    "        if qrel[0] == query_id :\n",
    "            for key, value in qrel[1].items():\n",
    "                relevant_docs.append(key)\n",
    "\n",
    "    ordered_results=[]\n",
    "    for query in dataset.items():\n",
    "        if query[0] == query_id:\n",
    "            ordered_results=get_results(get_query(query_id))\n",
    "            break\n",
    "    \n",
    "    for i, result in enumerate(ordered_results):\n",
    "        if result['_id'] in relevant_docs:\n",
    "            return 1 / (i+1)\n",
    "    \n",
    "    return 0\n",
    "\n",
    "queries_ids = {}\n",
    "for qrel in dataset.items():\n",
    "    queries_ids.update({qrel[0]:''})\n",
    "\n",
    "mrr_sum = 0\n",
    "for query_id in list(queries_ids.keys()):\n",
    "    mrr_sum += calculate_MRR(query_id)\n",
    "\n",
    "print(mrr_sum / len(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
